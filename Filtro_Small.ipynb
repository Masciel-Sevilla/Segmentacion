{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN4k8M5dhtzWYb5dhOFMDT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masciel-Sevilla/Segmentacion/blob/main/Filtro_Small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEnJv6kHaBNo",
        "outputId": "428d7e54-cb73-408b-e714-836712670e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lucasb-eyer/pydensecrf.git\n",
            "  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-hiqkpyg2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-req-build-hiqkpyg2\n",
            "  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 2723c7fa4f2ead16ae1ce3d8afe977724bb8f87f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'deformable_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modelo 'efficient_weed_model_S_best.keras' cargado exitosamente.\n",
            "üìñ Cargando datos del conjunto de TEST desde: ./Balanced/test/images\n",
            "\n",
            "üöÄ Iniciando evaluaci√≥n comparativa (Normal, CRF, TTA) en el conjunto de TEST...\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "üìä Resultados de Evaluaci√≥n en TEST para: efficient_weed_model_S_best.keras\n",
            "---------------------------------------------------------------------------\n",
            "Clase           | IoU Normal   | IoU con CRF  | IoU con TTA \n",
            "-----------------------------------------------------------------\n",
            "Background      | 0.9569       | 0.9482       | 0.9582      \n",
            "Cow-tongue      | 0.9032       | 0.8545       | 0.9043      \n",
            "Dandelion       | 0.9076       | 0.8378       | 0.9102      \n",
            "Kikuyo          | 0.9195       | 0.9053       | 0.9107      \n",
            "Other           | 0.7285       | 0.6854       | 0.7381      \n",
            "Potato          | 0.9018       | 0.8744       | 0.9042      \n",
            "-----------------------------------------------------------------\n",
            "mIoU Promedio   | 0.8862       | 0.8509       | 0.8876      \n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PASO 1: INSTALAR LIBRER√çAS (si no est√° instalada)\n",
        "# ==============================================================================\n",
        "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
        "\n",
        "# ==============================================================================\n",
        "# PASO 2: IMPORTAR LIBRER√çAS\n",
        "# ==============================================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral, create_pairwise_gaussian\n",
        "import traceback\n",
        "\n",
        "# ==============================================================================\n",
        "# PASO 3: CONFIGURACI√ìN Y RUTAS\n",
        "# ==============================================================================\n",
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "NUM_CLASSES = 6\n",
        "CLASS_NAMES = ['Background', 'Cow-tongue', 'Dandelion', 'Kikuyo', 'Other', 'Potato']\n",
        "MODEL_SAVE_PATH = 'efficient_weed_model_S_best.keras'\n",
        "BASE_PATH = './Balanced'\n",
        "# --- Se mantienen las rutas del conjunto de TEST para la evaluaci√≥n final ---\n",
        "EVAL_IMAGES_PATH = os.path.join(BASE_PATH, 'test/images')\n",
        "EVAL_MASKS_PATH = os.path.join(BASE_PATH, 'test/masks')\n",
        "\n",
        "# ==============================================================================\n",
        "# PASO 4: DEFINICIONES COMPLETAS PARA CARGAR EL MODELO S\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Clases personalizadas del modelo S ---\n",
        "class ASPPModule(layers.Layer):\n",
        "    def __init__(self, filters=192, **kwargs):\n",
        "        super(ASPPModule, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.conv_1x1 = layers.Conv2D(filters, 1, padding='same', use_bias=False)\n",
        "        self.bn_1x1 = layers.BatchNormalization()\n",
        "        self.relu_1x1 = layers.ReLU()\n",
        "        self.conv_3x3_6 = layers.Conv2D(filters, 3, padding='same', dilation_rate=6, use_bias=False)\n",
        "        self.bn_3x3_6 = layers.BatchNormalization()\n",
        "        self.relu_3x3_6 = layers.ReLU()\n",
        "        self.conv_3x3_12 = layers.Conv2D(filters, 3, padding='same', dilation_rate=12, use_bias=False)\n",
        "        self.bn_3x3_12 = layers.BatchNormalization()\n",
        "        self.relu_3x3_12 = layers.ReLU()\n",
        "        self.conv_3x3_18 = layers.Conv2D(filters, 3, padding='same', dilation_rate=18, use_bias=False)\n",
        "        self.bn_3x3_18 = layers.BatchNormalization()\n",
        "        self.relu_3x3_18 = layers.ReLU()\n",
        "        self.global_avg_pool = layers.GlobalAveragePooling2D(keepdims=True)\n",
        "        self.conv_1x1_gap = layers.Conv2D(filters, 1, padding='same', use_bias=False)\n",
        "        self.bn_1x1_gap = layers.BatchNormalization()\n",
        "        self.relu_1x1_gap = layers.ReLU()\n",
        "        self.conv_final = layers.Conv2D(filters, 1, padding='same', use_bias=False)\n",
        "        self.bn_final = layers.BatchNormalization()\n",
        "        self.relu_final = layers.ReLU()\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        conv_1x1 = self.relu_1x1(self.bn_1x1(self.conv_1x1(inputs), training=training))\n",
        "        conv_3x3_6 = self.relu_3x3_6(self.bn_3x3_6(self.conv_3x3_6(inputs), training=training))\n",
        "        conv_3x3_12 = self.relu_3x3_12(self.bn_3x3_12(self.conv_3x3_12(inputs), training=training))\n",
        "        conv_3x3_18 = self.relu_3x3_18(self.bn_3x3_18(self.conv_3x3_18(inputs), training=training))\n",
        "        gap = self.global_avg_pool(inputs)\n",
        "        gap = self.relu_1x1_gap(self.bn_1x1_gap(self.conv_1x1_gap(gap), training=training))\n",
        "        gap = tf.image.resize(gap, [input_shape[1], input_shape[2]], method='bilinear')\n",
        "        concat = layers.Concatenate()([conv_1x1, conv_3x3_6, conv_3x3_12, conv_3x3_18, gap])\n",
        "        output = self.relu_final(self.bn_final(self.conv_final(concat), training=training))\n",
        "        output = self.dropout(output, training=training)\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ASPPModule, self).get_config()\n",
        "        config.update({\"filters\": self.filters})\n",
        "        return config\n",
        "\n",
        "class DeformableAttention(layers.Layer):\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super(DeformableAttention, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.attention_conv = layers.Conv2D(self.filters, 1, padding='same', activation='sigmoid', name='attention_weights_conv', use_bias=False)\n",
        "        self.bn_attention = layers.BatchNormalization()\n",
        "        self.feature_conv = layers.SeparableConv2D(self.filters, 3, padding='same', name='feature_processing_conv', use_bias=False)\n",
        "        self.bn_feature = layers.BatchNormalization()\n",
        "        self.relu_feature = layers.ReLU()\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        attention_weights = self.bn_attention(self.attention_conv(inputs), training=training)\n",
        "        features = self.relu_feature(self.bn_feature(self.feature_conv(inputs), training=training))\n",
        "        attended_features = features * attention_weights\n",
        "        return attended_features\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(DeformableAttention, self).get_config()\n",
        "        config.update({\"filters\": self.filters})\n",
        "        return config\n",
        "\n",
        "# --- Funciones personalizadas (placeholders para la carga del modelo) ---\n",
        "def dice_coefficient(y_true, y_pred): return 0.0\n",
        "def dice_loss(y_true, y_pred): return 0.0\n",
        "def combined_loss(y_true, y_pred): return 0.0\n",
        "def iou_metric(y_true, y_pred): return 0.0\n",
        "\n",
        "# --- Funci√≥n IoU para la Evaluaci√≥n ---\n",
        "def iou_per_class(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    if y_pred.shape[-1] != NUM_CLASSES:\n",
        "        y_pred = tf.one_hot(tf.cast(y_pred, tf.int32), depth=NUM_CLASSES)\n",
        "    else:\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2]) - intersection\n",
        "    iou = tf.where(tf.equal(union, 0), 1.0, intersection / union)\n",
        "    return tf.squeeze(iou)\n",
        "\n",
        "# --- Funciones de Post-Procesamiento (CRF y TTA) ---\n",
        "def refine_segmentation_with_crf(image, softmax_output, crf_params):\n",
        "    image = np.ascontiguousarray(image)\n",
        "    unary = unary_from_softmax(np.ascontiguousarray(softmax_output))\n",
        "    d = dcrf.DenseCRF2D(image.shape[1], image.shape[0], softmax_output.shape[0])\n",
        "    d.setUnaryEnergy(unary)\n",
        "    d.addPairwiseEnergy(create_pairwise_gaussian(sdims=(crf_params['g_sdims'], crf_params['g_sdims']), shape=image.shape[:2]),\n",
        "                        compat=crf_params['g_compat'], kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "    d.addPairwiseEnergy(create_pairwise_bilateral(sdims=(crf_params['b_sdims'], crf_params['b_sdims']),\n",
        "                                                  schan=(crf_params['b_rgb_sdims'], crf_params['b_rgb_sdims'], crf_params['b_rgb_sdims']),\n",
        "                                                  img=image, chdim=2),\n",
        "                        compat=crf_params['b_compat'], kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "    Q = d.inference(crf_params['iterations'])\n",
        "    return np.argmax(Q, axis=0).reshape((image.shape[0], image.shape[1]))\n",
        "\n",
        "def perform_segmentation_tta(model, image_tensor):\n",
        "    all_predictions = []\n",
        "    pred_original = model.predict(image_tensor, verbose=0)\n",
        "    all_predictions.append(pred_original)\n",
        "    flipped_lr = tf.image.flip_left_right(image_tensor)\n",
        "    pred_flipped_lr = model.predict(flipped_lr, verbose=0)\n",
        "    all_predictions.append(tf.image.flip_left_right(pred_flipped_lr))\n",
        "    flipped_ud = tf.image.flip_up_down(image_tensor)\n",
        "    pred_flipped_ud = model.predict(flipped_ud, verbose=0)\n",
        "    all_predictions.append(tf.image.flip_up_down(pred_flipped_ud))\n",
        "    avg_preds = tf.reduce_mean(tf.stack(all_predictions), axis=0)\n",
        "    return avg_preds[0]\n",
        "\n",
        "# ==============================================================================\n",
        "# PASO 5: CARGAR DATOS Y EJECUTAR EVALUACI√ìN\n",
        "# ==============================================================================\n",
        "def load_dataset_for_evaluation(img_path, msk_path):\n",
        "    image_paths = sorted(glob(os.path.join(img_path, '*.jpg')))\n",
        "    mask_paths = sorted(glob(os.path.join(msk_path, '*.png')))\n",
        "    if not image_paths or not mask_paths:\n",
        "        raise FileNotFoundError(f\"No se encontraron im√°genes o m√°scaras en: {img_path}, {msk_path}\")\n",
        "    val_ds_original = tf.data.Dataset.from_tensor_slices(image_paths).map(lambda p: tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(p), channels=3), [IMG_HEIGHT, IMG_WIDTH]))\n",
        "    val_ds_processed = val_ds_original.map(tf.keras.applications.efficientnet_v2.preprocess_input)\n",
        "    val_ds_masks = tf.data.Dataset.from_tensor_slices(mask_paths).map(lambda p: tf.one_hot(tf.squeeze(tf.cast(tf.image.resize(tf.image.decode_png(tf.io.read_file(p), channels=1), [IMG_HEIGHT, IMG_WIDTH], method='nearest'), tf.int32)), depth=NUM_CLASSES))\n",
        "    return tf.data.Dataset.zip((val_ds_original, val_ds_processed, val_ds_masks)).batch(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    custom_objects = {\n",
        "        'combined_loss': combined_loss, 'dice_coefficient': dice_coefficient,\n",
        "        'iou_metric': iou_metric, 'dice_loss': dice_loss,\n",
        "        'ASPPModule': ASPPModule, 'DeformableAttention': DeformableAttention\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(MODEL_SAVE_PATH, custom_objects=custom_objects)\n",
        "        print(f\"‚úÖ Modelo '{MODEL_SAVE_PATH}' cargado exitosamente.\")\n",
        "\n",
        "        print(f\"üìñ Cargando datos del conjunto de TEST desde: {EVAL_IMAGES_PATH}\")\n",
        "        eval_dataset = load_dataset_for_evaluation(EVAL_IMAGES_PATH, EVAL_MASKS_PATH)\n",
        "\n",
        "        # --- Listas para almacenar los resultados de los TRES m√©todos ---\n",
        "        per_class_iou_normal_list, per_class_iou_crf_list, per_class_iou_tta_list = [], [], []\n",
        "\n",
        "        crf_params = {\n",
        "            'g_sdims': 3, 'g_compat': 3, 'b_sdims': 80,\n",
        "            'b_rgb_sdims': 13, 'b_compat': 10, 'iterations': 10\n",
        "        }\n",
        "\n",
        "        print(\"\\nüöÄ Iniciando evaluaci√≥n comparativa (Normal, CRF, TTA) en el conjunto de TEST...\")\n",
        "\n",
        "        num_images = len(glob(os.path.join(EVAL_IMAGES_PATH, '*.jpg')))\n",
        "        for i, (image_original, image_processed, mask_true_one_hot) in enumerate(eval_dataset):\n",
        "            print(f\"    Procesando imagen {i+1}/{num_images}...\", end='\\r')\n",
        "            image_uint8 = tf.cast(image_original[0], tf.uint8).numpy()\n",
        "\n",
        "            # --- Predicci√≥n base ---\n",
        "            base_probabilities = model.predict(image_processed, verbose=0)[0]\n",
        "\n",
        "            # --- 1. Predicci√≥n Normal (sin filtros) ---\n",
        "            mask_pred_normal = np.argmax(base_probabilities, axis=-1)\n",
        "            iou_normal = iou_per_class(mask_true_one_hot, mask_pred_normal[np.newaxis, ...])\n",
        "            per_class_iou_normal_list.append(iou_normal.numpy())\n",
        "\n",
        "            # --- 2. Predicci√≥n con Post-procesamiento CRF ---\n",
        "            probs_for_crf = base_probabilities.transpose(2, 0, 1)\n",
        "            mask_pred_crf = refine_segmentation_with_crf(image_uint8, probs_for_crf, crf_params)\n",
        "            iou_crf = iou_per_class(mask_true_one_hot, mask_pred_crf[np.newaxis, ...])\n",
        "            per_class_iou_crf_list.append(iou_crf.numpy())\n",
        "\n",
        "            # --- 3. Predicci√≥n con Test-Time Augmentation (TTA) ---\n",
        "            probs_pred_tta = perform_segmentation_tta(model, image_processed)\n",
        "            mask_pred_tta = np.argmax(probs_pred_tta, axis=-1)\n",
        "            iou_tta = iou_per_class(mask_true_one_hot, mask_pred_tta[np.newaxis, ...])\n",
        "            per_class_iou_tta_list.append(iou_tta.numpy())\n",
        "\n",
        "        # Calcular promedios finales y mostrar la tabla de resultados\n",
        "        avg_iou_normal = np.mean(per_class_iou_normal_list, axis=0)\n",
        "        avg_iou_crf = np.mean(per_class_iou_crf_list, axis=0)\n",
        "        avg_iou_tta = np.mean(per_class_iou_tta_list, axis=0)\n",
        "\n",
        "        print(\"\\n\\n\" + \"---\" * 25)\n",
        "        print(f\"üìä Resultados de Evaluaci√≥n en TEST para: {MODEL_SAVE_PATH}\")\n",
        "        print(\"---\" * 25)\n",
        "        print(f\"{'Clase':<15} | {'IoU Normal':<12} | {'IoU con CRF':<12} | {'IoU con TTA':<12}\")\n",
        "        print(\"-\" * 65)\n",
        "        for i, class_name in enumerate(CLASS_NAMES):\n",
        "            print(f\"{class_name:<15} | {avg_iou_normal[i]:<12.4f} | {avg_iou_crf[i]:<12.4f} | {avg_iou_tta[i]:<12.4f}\")\n",
        "\n",
        "        print(\"-\" * 65)\n",
        "        mIoU_normal = np.mean(avg_iou_normal)\n",
        "        mIoU_crf = np.mean(avg_iou_crf)\n",
        "        mIoU_tta = np.mean(avg_iou_tta)\n",
        "        print(f\"{'mIoU Promedio':<15} | {mIoU_normal:<12.4f} | {mIoU_crf:<12.4f} | {mIoU_tta:<12.4f}\")\n",
        "        print(\"---\" * 25)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR: Ocurri√≥ un problema durante la ejecuci√≥n: {e}\")\n",
        "        traceback.print_exc()"
      ]
    }
  ]
}